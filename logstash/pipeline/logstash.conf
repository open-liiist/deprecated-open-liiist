#logstash.conf
input {
  tcp {
    port => 50000
    codec => json_lines { target => "message" } #Target per evitare conflitti
  }
  jdbc {
    jdbc_connection_string => "jdbc:postgresql://34.154.78.187:5432/list_db"
    jdbc_user => "${JDBC_USER}"
    jdbc_password => "${JDBC_PASSWORD}"
    jdbc_driver_library => "/usr/share/logstash/drivers/postgresql-42.5.4.jar"
    jdbc_driver_class => "org.postgresql.Driver"
    statement_filepath => "/usr/share/logstash/pipeline/sql_query.sql"
    use_column_value => true
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    schedule => "* * * * *"  # Esegue ogni minuto
    last_run_metadata_path => "/usr/share/logstash/pipeline/.logstash_jdbc_last_run"
  }
}

filter {
  mutate {
    rename => { "lng" => "lon" }
  }

  mutate {
    convert => { "lat" => "float" }
    convert => { "lon" => "float" }
  }

  mutate {
    remove_field => ["created_at"] # Rimosso il campo created_at
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "products"  # Puntando all'alias
    document_id => "%{id}"
    doc_as_upsert => true
  }
  stdout { codec => rubydebug }
}
