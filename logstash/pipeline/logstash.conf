#logstash.conf
input {
  tcp {
    port => 50000
    codec => json_lines { target => "message" } # Target per evitare conflitti
  }
  jdbc {
    jdbc_connection_string => "jdbc:postgresql://db:5432/appdb"
    jdbc_user => "${JDBC_USER}"
    jdbc_password => "${JDBC_PASSWORD}"
    jdbc_driver_library => "/usr/share/logstash/drivers/postgresql-42.5.4.jar"
    jdbc_driver_class => "org.postgresql.Driver"
    statement_filepath => "/usr/share/logstash/pipeline/sql_query.sql"
    use_column_value => true
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    schedule => "* * * * *"  # Esegue ogni minuto
    last_run_metadata_path => "/usr/share/logstash/pipeline/.logstash_jdbc_last_run"
    #clean_run => true #Con questa opzione, Logstash ignorerÃ  il contenuto del file di tracking e reindicizzerÃ  tutti i dati al prossimo (ogni) avvio.
  }
}

filter {
  mutate {
    rename => { "lng" => "lon" }
  }

  mutate {
    convert => { "lat" => "float" }
    convert => { "lon" => "float" }
  }
  
  ruby {
    code => '
      # Proviamo prima a leggere "lat" e "lon" dal livello principale
      lat = event.get("lat")
      lon = event.get("lon")
      
      # Se sono nulli, proviamo ad estrarli dal campo "message"
      if lat.nil? || lon.nil?
        msg = event.get("message")
        if msg.is_a?(Hash)
          lat = msg["lat"]
          lon = msg["lng"]
        end
      end
      
      # Se abbiamo ottenuto i valori, li usiamo per creare il campo "location"
      if lat && lon
        event.set("location", { "lat" => lat, "lon" => lon })
      else
        event.tag("missing_location")
      end
    '
  }
  
  mutate {
    remove_field => ["created_at"] # Rimuove il campo created_at se non necessario
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "products"  # Puntando all'alias
    document_id => "%{id}"
    doc_as_upsert => true
  }
  stdout { codec => rubydebug }
}
