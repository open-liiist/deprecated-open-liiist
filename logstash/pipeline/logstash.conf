# logstash.conf
input {
  tcp {
    port => 50000
    codec => json_lines { target => "message" } # Target to avoid conflicts
  }
  jdbc {
    jdbc_connection_string => "jdbc:postgresql://db:5432/appdb"
    jdbc_user => "${JDBC_USER}"
    jdbc_password => "${JDBC_PASSWORD}"
    jdbc_driver_library => "/usr/share/logstash/drivers/postgresql-42.5.4.jar"
    jdbc_driver_class => "org.postgresql.Driver"
    statement_filepath => "/usr/share/logstash/pipeline/sql_query.sql"
    use_column_value => true
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    schedule => "* * * * *"  # Runs every minute
    last_run_metadata_path => "/usr/share/logstash/pipeline/.logstash_jdbc_last_run"
    #clean_run => true  # With this option, Logstash will ignore the tracking file content and re-index all data on the next run.
  }
}

filter {
  mutate {
    rename => { "lng" => "lon" }
  }

  mutate {
    convert => { "lat" => "float" }
    convert => { "lon" => "float" }
  }
  
  ruby {
    code => '
      # First try to read "lat" and "lon" from the top level
      lat = event.get("lat")
      lon = event.get("lon")
      
      # If they are null, try extracting them from the "message" field
      if lat.nil? || lon.nil?
        msg = event.get("message")
        if msg.is_a?(Hash)
          lat = msg["lat"]
          lon = msg["lng"]
        end
      end
      
      # If we retrieved the values, use them to create the "location" field
      if lat && lon
        event.set("location", { "lat" => lat, "lon" => lon })
      else
        event.tag("missing_location")
      end
    '
  }
  
  mutate {
    remove_field => ["created_at"] # Remove the 'created_at' field if not needed
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "products"  # Pointing to the alias
    document_id => "%{id}"
    doc_as_upsert => true
  }
  stdout { codec => rubydebug }
}
